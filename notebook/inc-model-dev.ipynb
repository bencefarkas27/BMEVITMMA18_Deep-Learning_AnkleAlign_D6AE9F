{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "8a3f8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "29847982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name=__name__):\n",
    "    \"\"\"\n",
    "    Sets up a logger that outputs to the console (stdout).\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "9d93a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-12 13:56:58,296 - INFO - Training images shape: torch.Size([241, 1, 224, 224])\n",
      "2025-12-12 13:56:58,298 - INFO - Training labels shape: torch.Size([241])\n",
      "2025-12-12 13:56:58,299 - INFO - Label mapping: {np.str_('1_Pronacio'): 0, np.str_('2_Neutralis'): 1, np.str_('3_Szupinacio'): 2}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_folder = \"../data\"\n",
    "preped_folder = os.path.join(data_folder, \"_preped\")\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(data_folder, 'train_data.csv')).values.tolist()\n",
    "test_data = pd.read_csv(os.path.join(data_folder, 'test_data.csv')).values.tolist()\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to consistent size\n",
    "    transforms.ToTensor(),           # Convert to tensor [0, 1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for img_name, label in train_data:\n",
    "    img_path = os.path.join(preped_folder, img_name)\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('L') # Convert to grayscale\n",
    "        img_tensor = transform(img)\n",
    "        x_train.append(img_tensor)\n",
    "        y_train.append(label)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error loading {img_name}: {e}\")\n",
    "\n",
    "# Stack into tensors\n",
    "x_train_tensor = torch.stack(x_train)\n",
    "logger.info(f\"Training images shape: {x_train_tensor.shape}\")\n",
    "\n",
    "# Encode labels to integers\n",
    "label_to_idx = {label: idx for idx, label in enumerate(np.unique(y_train))}\n",
    "y_train_encoded = [label_to_idx[label] for label in y_train]\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "\n",
    "logger.info(f\"Training labels shape: {y_train_tensor.shape}\")\n",
    "logger.info(f\"Label mapping: {label_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "f39e88ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-12 13:57:04,754 - INFO - Test images shape: torch.Size([49, 1, 224, 224])\n",
      "2025-12-12 13:57:04,755 - INFO - Test labels shape: torch.Size([49])\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for img_name, label in test_data:\n",
    "    img_path = os.path.join(preped_folder, img_name)\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('L') # Convert to grayscale\n",
    "        img_tensor = transform(img)\n",
    "        x_test.append(img_tensor)\n",
    "        y_test.append(label)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error loading {img_name}: {e}\")\n",
    "\n",
    "x_test_tensor = torch.stack(x_test)\n",
    "logger.info(f\"Test images shape: {x_test_tensor.shape}\")\n",
    "y_test_encoded = [label_to_idx[label] for label in y_test]\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "logger.info(f\"Test labels shape: {y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "5f6197a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-12 13:57:04,766 - INFO - CUDA available: True\n",
      "2025-12-12 13:57:04,766 - INFO - Number of GPUs: 1\n",
      "2025-12-12 13:57:04,767 - INFO - \n",
      "GPU 0: NVIDIA GeForce RTX 4060\n",
      "2025-12-12 13:57:04,768 - INFO -   Memory: 8.00 GB\n",
      "2025-12-12 13:57:04,769 - INFO -   Compute Capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        logger.info(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        logger.info(f\"  Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        logger.info(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    logger.info(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "e2a044d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 70\n",
    "device = 'cuda' \n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "c1a2c812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Win 10\\_netrc\n"
     ]
    }
   ],
   "source": [
    "# wandb login an init\n",
    "# Login to wandb with API key\n",
    "load_dotenv()\n",
    "wandb.login(key=os.getenv(\"wandbKey\"))\n",
    "\n",
    "def init_wandb():\n",
    "    # Initialize wandb project\n",
    "    wandb.init(\n",
    "        project=\"ankle-align\",\n",
    "        config={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"Custom CNN\",\n",
    "            \"dataset\": \"AnkleAlign\",\n",
    "            \"optimizer\": \"Adam\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "bd9b01ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [16, 3]                   --\n",
       "├─Conv2d: 1-1                            [16, 8, 112, 112]         80\n",
       "├─ReLU: 1-2                              [16, 8, 112, 112]         --\n",
       "├─Conv2d: 1-3                            [16, 16, 56, 56]          1,168\n",
       "├─ReLU: 1-4                              [16, 16, 56, 56]          --\n",
       "├─Conv2d: 1-5                            [16, 32, 28, 28]          4,640\n",
       "├─ReLU: 1-6                              [16, 32, 28, 28]          --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [16, 32, 1, 1]            --\n",
       "├─Flatten: 1-8                           [16, 32]                  --\n",
       "├─Linear: 1-9                            [16, 128]                 4,224\n",
       "├─ReLU: 1-10                             [16, 128]                 --\n",
       "├─Linear: 1-11                           [16, 64]                  8,256\n",
       "├─ReLU: 1-12                             [16, 64]                  --\n",
       "├─Linear: 1-13                           [16, 3]                   195\n",
       "==========================================================================================\n",
       "Total params: 18,563\n",
       "Trainable params: 18,563\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 133.07\n",
       "==========================================================================================\n",
       "Input size (MB): 3.21\n",
       "Forward/backward pass size (MB): 22.50\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 25.79\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net0 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),   # 224x224 -> 112x112\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),  # 112x112 -> 56x56\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # 56x56 -> 28x28\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.AdaptiveAvgPool2d(1),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 3)                       # Output layer     \n",
    ").to(device)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "net0.apply(init_weights)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net0.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net0.parameters(), lr=0.01)\n",
    "\n",
    "summary(net0, input_size=(batch_size, 1, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "e528be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Win 10\\Desktop\\BME\\DeepL\\BMEVITMMA18_Deep-Learning_AnkleAlign_D6AE9F\\notebook\\wandb\\run-20251212_135706-mfy8ex0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align/runs/mfy8ex0f' target=\"_blank\">colorful-sky-83</a></strong> to <a href='https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align' target=\"_blank\">https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align/runs/mfy8ex0f' target=\"_blank\">https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align/runs/mfy8ex0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 100%|██████████| 70/70 [00:00<00:00, 122.92it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss</td><td>0.62023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-sky-83</strong> at: <a href='https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align/runs/mfy8ex0f' target=\"_blank\">https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align/runs/mfy8ex0f</a><br> View project at: <a href='https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align' target=\"_blank\">https://wandb.ai/bencefarkas-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/ankle-align</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_135706-mfy8ex0f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0397367477416992, 1.3844525814056396, 0.9132133722305298, 0.9917631149291992, 0.9320054054260254, 0.8967487812042236, 0.8918147087097168, 0.8908231854438782, 0.8864966034889221, 0.866675853729248, 0.8517279028892517, 0.843503475189209, 0.8373140692710876, 0.8291265368461609, 0.820592999458313, 0.8140286207199097, 0.8116449117660522, 0.8017197251319885, 0.7955611944198608, 0.794366717338562, 0.7864888906478882, 0.7819048166275024, 0.7766679525375366, 0.7729988098144531, 0.7650701403617859, 0.7563906908035278, 0.748356819152832, 0.7373569011688232, 0.7254234552383423, 0.712177574634552, 0.6971168518066406, 0.689474880695343, 0.7014814019203186, 0.6802625060081482, 0.6465932130813599, 0.6284670829772949, 0.6367696523666382, 0.6178879141807556, 0.5825920104980469, 0.5750789046287537, 0.5960018634796143, 0.6158249378204346, 0.5524513721466064, 0.5541458129882812, 0.626255452632904, 0.5536177754402161, 0.5403266549110413, 0.5101431012153625, 0.4849310517311096, 0.48394930362701416, 0.46147555112838745, 0.42652204632759094, 0.43707549571990967, 0.3818701505661011, 0.3881639838218689, 0.35097187757492065, 0.34799113869667053, 0.3119945824146271, 0.3133845031261444, 0.2815743088722229, 0.2683374881744385, 0.23992353677749634, 0.22841417789459229, 0.1999947726726532, 0.18712551891803741, 0.1996021568775177, 0.3880300223827362, 0.788131594657898, 0.19387337565422058, 0.6202319860458374]\n"
     ]
    }
   ],
   "source": [
    "# Trying to overfit one batch\n",
    "init_wandb()\n",
    "one_batch = next(iter(train_loader))\n",
    "images, labels = one_batch\n",
    "\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "loss_values = []\n",
    "net0.train()\n",
    "for epoch in tqdm(range(num_epochs), desc='Training model'):\n",
    "        pred_logits = net0(images)\n",
    "        loss = loss_fn(pred_logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_values.append(loss.item())\n",
    "        wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": loss.item()\n",
    "            })\n",
    "        \n",
    "wandb.finish()\n",
    "print(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "a9f0ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, optimizer, loss_fn, enable_early_stopping=False, patience=5):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    if enable_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    network.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training model'):\n",
    "        network.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, target_labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            target_labels = target_labels.to(device)\n",
    "\n",
    "            pred_logits = network(images)\n",
    "            loss = loss_fn(pred_logits, target_labels)\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "\n",
    "        if enable_early_stopping:\n",
    "            network.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, target_labels in val_loader:\n",
    "                    images = images.to(device)\n",
    "                    target_labels = target_labels.to(device)\n",
    "                    \n",
    "                    pred_logits = network(images)\n",
    "                    loss = loss_fn(pred_logits, target_labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    _, predicted = torch.max(pred_logits, 1)\n",
    "                    total += target_labels.size(0)\n",
    "                    correct += (predicted == target_labels).sum().item()\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "        # Log metrics\n",
    "        if enable_early_stopping:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy\n",
    "            })\n",
    "        else:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_train_loss\n",
    "            })\n",
    "        loss_values.append(avg_train_loss)\n",
    "        \n",
    "        if enable_early_stopping:\n",
    "            logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        else:\n",
    "            logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if enable_early_stopping:\n",
    "            early_stopping(avg_val_loss, network)\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping triggered\")\n",
    "                network.load_state_dict(early_stopping.best_model)\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if enable_early_stopping and early_stopping.best_model is not None:\n",
    "        network.load_state_dict(early_stopping.best_model)\n",
    "        logger.info(\"Loaded best model weights\")\n",
    "\n",
    "    logger.info(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "517e5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(network):\n",
    "    # Training score\n",
    "    true_labels = y_test_encoded\n",
    "    predicted_labels = []\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = np.mean([true == pred for true, pred in zip(true_labels, predicted_labels)])\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    logger.info(f\"network accuracy: {accuracy * 100:.2f}%\")\n",
    "    logger.info(f\"network precision: {precision * 100:.2f}%\")\n",
    "    logger.info(f\"network recall: {recall * 100:.2f}%\")\n",
    "    logger.info(f\"network F1 score: {f1 * 100:.2f}%\")\n",
    "\n",
    "    logger.info(f\"Detailed Classification Report: \\n{classification_report(true_labels, predicted_labels)}\")\n",
    "\n",
    "    # Log test metrics\n",
    "    wandb.log({\n",
    "        \"test_accuracy\": accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"test_f1\": f1\n",
    "    })\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLankle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
